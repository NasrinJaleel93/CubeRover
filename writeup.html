

    <section class="main-content">
      <h2>
<a id="welcome-to-github-pages" class="anchor" href="#welcome-to-github-pages" aria-hidden="true"><span class="octicon octicon-link"></span></a><b>Writeup</b></h2>

<h3>CubeRover</h3>
 

<p> The CubeRover is a semi-autonomous Moon rover. It’s mission is to traverse a two-kilometer circuit on the surface of the Moon. The CubeRover is constrained by size, weight, power and cost (SWaP-C constraints). The CubeRover must fit in a 30cm3 box, weigh no more that 10 kilograms, is not self-charging and must complete the circuit on a single charge.</p>

<h3>
<a id="designer-templates" class="anchor" href="#designer-templates" aria-hidden="true"><span class="octicon octicon-link"></span></a>Hardware</h3>
<p>The NVIDIA Tegra K, is a complete mobile system-on-a-chip sports a single Kepler SMX with 192 CUDA cores. It is a 32-bit variant quad-core ARM Cortex-A15 MPCore R3 along with a low power companion core. We use Kinect v1 to capture RGB and depth images.
</p>

<h3>
<a id="rather-drive-stick" class="anchor" href="#rather-drive-stick" aria-hidden="true"><span class="octicon octicon-link"></span></a>Software</h3>
<p>We use OpenCV’s implementation of RGB-D camera tracking found in the contrib module as the basis for our parallel implementation, as well as for performance and correctness checking. We also use a few other libraries like libfreenect to interface the Kinect with OpenCV.
</p>

<h3>
<a id="authors-and-contributors" class="anchor" href="#authors-and-contributors" aria-hidden="true"><span class="octicon octicon-link"></span></a>Visual Odometry</h3>

<p>
  In Computer Vision, visual odometry (we’ll call this camera tracking from here) is the process of determining the position and orientation of a robot by analyzing the associated camera images. In our algorithm we get as input a stream of RGB and depth images from the Kinect, and we try to estimate the camera motion between each pair of frames.
</p>

<h3>
<a id="authors-and-contributors" class="anchor" href="#authors-and-contributors" aria-hidden="true"><span class="octicon octicon-link"></span></a>Sparse vs Dense Tracking</h3>
<p>There are two main variations of camera tracking - sparse tracking and dense tracking.
</p>
<p>
In the case of sparse tracking, a feature detection algorithm is run on each of the images to extract a handful of features (the number of features is typically much less than the total number of pixels). These two sets of features are then explored for correspondences between the images. 
</p>
<p>
In dense tracking, we calculate the correspondences between two consecutive frames by comparing all the pixels. Although, this is computationally very expensive, it is shown to give better accuracy. 
Most algorithms that are available in real-time today use sparse feature detection. We have implemented dense feature tracking in our algorithm to exploit the parallelism available on the Tegra and achieve better accuracy.
</p>
<h3>
<a id="authors-and-contributors" class="anchor" href="#authors-and-contributors" aria-hidden="true"><span class="octicon octicon-link"></span></a>Challenges</h3>
<p>Ideally, we’d like to get the algorithm in real-time. Real-time camera tracking would make it possible to perform odometry not only in fast-moving robots, but also in hand-held camera systems. OpenCV’s implementation takes about 750 milliseconds to estimate the rotation and translation between two frames. Almost no application can use their algorithm to run tracking in real-time.
</p>

<p>
The algorithm refines motion parameters iteratively. This creates a lot of dependencies between different parts of the code, and makes large parts of the code inherently serial. As we shall see in future sections, not every parallelizable part of the algorithm is embarrassingly so. There are a few places where SIMD divergence considerations (among other things) force us to sacrifice parallelism in the code. Any simple filter or mask that only considers a scattering of pixels across the image already poses a difficulty in terms of divergent computation.
</p>
<p>
Another goal for us was to be able to let our code merge seamlessly with OpenCV’s existing RGB-D odometry API.</p>
</p>
<h3>
<a id="authors-and-contributors" class="anchor" href="#authors-and-contributors" aria-hidden="true"><span class="octicon octicon-link"></span></a>Preprocessing Images</h3>

<p>
  The first step of the algorithm involves processing RGB and depth images. 
<font face = "Courier New">gpuPreprocessDepth()</font> processes every depth image and only depths that range in a certain threshold are considered for computation. This is done to avoid computing the shift in depths of pixels that are very far away since their movement is significantly lower compared to the ones that are closer to the camera. We have launched a Cuda kernel to do this since a single instruction is executed in parallel across all the pixels of the image. This involves a slight bit of divergence due to the threshold check. We assign Nan values to all the pixels whose depths are either negative or greater than a certain maximum depth.
</p>

<p>
gpuBuildPyramids() processes all the rgb images, i.e., the images captured at time t and the images captured at time t+1. The main functionality here is to find large and small features in the images and compare them against each other . Large image features can be computed by convolving the entire image with a Gaussian kernel and downsampling the image. Similarly, the smaller image features are estimated by convolving the original images with a Gaussian kernel [r]. In other words, downsampling the images to store a pyramid of images (different sizes of the downsampled/smoothened images belong to different pyramid levels)
</p>

<p>
gpuSobel() calculates the gradient images of every downsampled pyramid images of images taken at time t+1. Sobel filter [r] is used in computing the partial derivative along both the x and the y axis. This is done by convolving every image pixel with a 2D kernel given by the matrices:
</p>

<h3>
<a id="authors-and-contributors" class="anchor" href="#authors-and-contributors" aria-hidden="true"><span class="octicon octicon-link"></span></a>References<a name="ref"></a></h3>
<ol type="1">
<li>Engel, J., Sturm, J., Cremers, D.: Semi-dense visual odometry for a monocular camera. In: Intl. Conf. on Computer Vision (ICCV) (2013)</li>
<li>T. Whelan, H. Johannsson, M. Kaess, J. Leonard, and J. McDonald, "Robust real-time visual odometry for dense RGB-D mapping," in IEEE Intl. Conf. on Robotics and Automation (ICRA), Karlsruhe, Germany, 2013.</li>
<li>F. Steinbrücker, J. Sturm, and D. Cremers, "Real-time visual odometry from dense RGB-D images," in ICCV Workshop on Live Dense Reconstruction with Moving Cameras, 2011</li>
<li>Kerl, C., Sturm, J., Cremers, D.: Dense visual SLAM for RGB-D cameras. In: Intl. Conf. on Intelligent Robot Systems (IROS) (2013)</li>
<li>J. Engel, T. Scho ̈ps, and D. Cremers. LSD-SLAM: Large-scale direct monocular SLAM. In European Conference on Computer Vision (ECCV), September 2014.</li>
</ol>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/uprasad/CubeRover">CubeRover</a> is maintained by <a href="https://github.com/uprasad">uprasad</a> & <a href="https://github.com/nikithashr">nikithashr</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>
