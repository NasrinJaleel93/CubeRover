

    <section class="main-content">
      <h2>
<a id="welcome-to-github-pages" class="anchor" href="#welcome-to-github-pages" aria-hidden="true"><span class="octicon octicon-link"></span></a><b>Writeup</b></h2>

<h3>CubeRover</h3>
 

<p> The CubeRover is a semi-autonomous Moon rover. It’s mission is to traverse a two-kilometer circuit on the surface of the Moon. The CubeRover is constrained by size, weight, power and cost (SWaP-C constraints). The CubeRover must fit in a 30cm3 box, weigh no more that 10 kilograms, is not self-charging and must complete the circuit on a single charge.</p>

<h3>
<a id="designer-templates" class="anchor" href="#designer-templates" aria-hidden="true"><span class="octicon octicon-link"></span></a>Hardware</h3>
<p>The NVIDIA Tegra K, is a complete mobile system-on-a-chip sports a single Kepler SMX with 192 CUDA cores. It is a 32-bit variant quad-core ARM Cortex-A15 MPCore R3 along with a low power companion core. We use Kinect v1 to capture RGB and depth images.
</p>

<h3>
<a id="rather-drive-stick" class="anchor" href="#rather-drive-stick" aria-hidden="true"><span class="octicon octicon-link"></span></a>Software</h3>
<p>We use OpenCV’s implementation of RGB-D camera tracking found in the contrib module as the basis for our parallel implementation, as well as for performance and correctness checking. We also use a few other libraries like libfreenect to interface the Kinect with OpenCV.
</p>

<h3>
<a id="authors-and-contributors" class="anchor" href="#authors-and-contributors" aria-hidden="true"><span class="octicon octicon-link"></span></a>Visual Odometry</h3>

<p>
  In Computer Vision, visual odometry (we’ll call this camera tracking from here) is the process of determining the position and orientation of a robot by analyzing the associated camera images. In our algorithm we get as input a stream of RGB and depth images from the Kinect, and we try to estimate the camera motion between each pair of frames.
</p>

<h3>
<a id="authors-and-contributors" class="anchor" href="#authors-and-contributors" aria-hidden="true"><span class="octicon octicon-link"></span></a>Sparse vs Dense Tracking</h3>
<p>There are two main variations of camera tracking - sparse tracking and dense tracking.
</p>
<p>
In the case of sparse tracking, a feature detection algorithm is run on each of the images to extract a handful of features (the number of features is typically much less than the total number of pixels). These two sets of features are then explored for correspondences between the images. 
</p>
<p>
In dense tracking, we calculate the correspondences between two consecutive frames by comparing all the pixels. Although, this is computationally very expensive, it is shown to give better accuracy. 
Most algorithms that are available in real-time today use sparse feature detection. We have implemented dense feature tracking in our algorithm to exploit the parallelism available on the Tegra and achieve better accuracy.
</p>
<h3>
<a id="authors-and-contributors" class="anchor" href="#authors-and-contributors" aria-hidden="true"><span class="octicon octicon-link"></span></a>Stretch Goal</h3>
<p>As mentioned earlier, depth map estimation is a non-trivial problem [<a href="#ref">5</a>] [<a href="#ref">1</a>]. In the absence of an RGB-D sensor in the CubeRover, we would have to generate depth maps using monocular camera only. Our stretch goal would be to implement a depth map estimation algorithm and GPU/CPU parallelize it.</p>

<h3>
<a id="authors-and-contributors" class="anchor" href="#authors-and-contributors" aria-hidden="true"><span class="octicon octicon-link"></span></a>Schedule</h3>

<table class="pure-table pure-table-horizontal">
    <thead>
        <tr bgcolor="#f0f0f0">
            <th>#</th>
            <th>Date</th>
            <th>Objective</th>
            <th>Description</th>
        </tr>
    </thead>

    <tbody>
        <tr>
            <td>1</td>
            <td nowrap= "nowrap">April 4</td>
            <td>Setup the working environment </td>
            <td>Set up the tegra and kinect. Install the required software packages</td>
        </tr>

        <tr bgcolor="#f0f0f0">
            <td>2</td>
            <td nowrap= "nowrap">April 6</td>
            <td>Initial testing of the benchmarked software</td>
            <td></td>
        </tr>

        <tr>
            <td>3*</td>
            <td nowrap="nowrap">April 20</td>
            <td>Visual Odometry GPU Implementation</td>
            <td></td>
        </tr>

        <tr bgcolor="#f0f0f0">
            <td>4*</td>
            <td nowrap="nowrap">April 25</td>
            <td>Visual Odometry CPU Implementation</td>
            <td></td>
        </tr>

        <tr>
            <td>5</td>
            <td nowrap="nowrap">April 26 - May 1</td>
            <td>Exam Preparation</td>
            <td></td>
        </tr>

        <tr bgcolor="#f0f0f0">
            <td>6</td>
            <td nowrap="nowrap">May 2 - May 5</td>
            <td>Integration</td>
            <td>Integrating all the modules and testing for correctness and performance. Also, testing on different test data sets</td>
        </tr>

        <tr>
            <td>7</td>
            <td nowrap="nowrap">May 6 - May 7</td>
            <td>Test for energy consumption</td>
            <td></td>
        </tr>

        <tr bgcolor="#f0f0f0">
            <td>8</td>
            <td nowrap="nowrap">May 7 - May 11</td>
            <td>Final Product</td>
            <td>Final report, presentation and an attempt at the stretch goal</td>
        </tr>
    </tbody>
</table>
<font size = "3"> <i> *Revised schedule</i></font>

<h3>
<a id="authors-and-contributors" class="anchor" href="#authors-and-contributors" aria-hidden="true"><span class="octicon octicon-link"></span></a>References<a name="ref"></a></h3>
<ol type="1">
<li>Engel, J., Sturm, J., Cremers, D.: Semi-dense visual odometry for a monocular camera. In: Intl. Conf. on Computer Vision (ICCV) (2013)</li>
<li>T. Whelan, H. Johannsson, M. Kaess, J. Leonard, and J. McDonald, "Robust real-time visual odometry for dense RGB-D mapping," in IEEE Intl. Conf. on Robotics and Automation (ICRA), Karlsruhe, Germany, 2013.</li>
<li>F. Steinbrücker, J. Sturm, and D. Cremers, "Real-time visual odometry from dense RGB-D images," in ICCV Workshop on Live Dense Reconstruction with Moving Cameras, 2011</li>
<li>Kerl, C., Sturm, J., Cremers, D.: Dense visual SLAM for RGB-D cameras. In: Intl. Conf. on Intelligent Robot Systems (IROS) (2013)</li>
<li>J. Engel, T. Scho ̈ps, and D. Cremers. LSD-SLAM: Large-scale direct monocular SLAM. In European Conference on Computer Vision (ECCV), September 2014.</li>
</ol>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/uprasad/CubeRover">CubeRover</a> is maintained by <a href="https://github.com/uprasad">uprasad</a> & <a href="https://github.com/nikithashr">nikithashr</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>
